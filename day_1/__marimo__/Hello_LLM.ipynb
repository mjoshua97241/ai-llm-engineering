{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {},
   "source": [
    "# AI/LLM Engineering Kick-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {},
   "source": [
    "For our initial activity, we will be using the OpenAI Library to Programmatically Access GPT-4.1-nano!\n",
    "\n",
    "In order to get started, you'll need an OpenAI Key. [here](https://platform.openai.com)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# Read API key from OpenAI_key.txt file\n",
    "with open(\"OpenAI_key.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {},
   "source": [
    "### Our First Prompt\n",
    "\n",
    "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
    "\n",
    "Let's create a `ChatCompletion` model to kick things off!\n",
    "\n",
    "There are three \"roles\" available to use:\n",
    "\n",
    "- `developer`\n",
    "- `assistant`\n",
    "- `user`\n",
    "\n",
    "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
    "\n",
    "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
    "\n",
    "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"markdown prose dark:prose-invert\"><span class=\"paragraph\">Great question! LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate building language model-powered applications, but they serve different primary purposes and have distinct features.</span>\n",
       "<span class=\"paragraph\"><strong>LangChain:</strong></span>\n",
       "<ul>\n",
       "<li><strong>Purpose:</strong> Primarily focused on enabling the development of complex chatbots, conversational agents, and applications that involve chaining multiple language model (LLM) tasks together.</li>\n",
       "<li>\n",
       "<span class=\"paragraph\"><strong>Key Features:</strong></span>\n",
       "<ul>\n",
       "<li>Provides tools for composing prompts, managing conversational memory, and orchestrating multi-step workflows.</li>\n",
       "<li>Supports integrations with various language models and APIs.</li>\n",
       "<li>Offers components like chains, prompts, memory, and agents to build dynamic applications.</li>\n",
       "<li>Well-suited for building interactive conversational systems and applications that require state management.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ul>\n",
       "<span class=\"paragraph\"><strong>LlamaIndex (GPT Index):</strong></span>\n",
       "<ul>\n",
       "<li><strong>Purpose:</strong> Designed to simplify the process of connecting large language models with external data sources, such as documents, knowledge bases, or custom data repositories.</li>\n",
       "<li>\n",
       "<span class=\"paragraph\"><strong>Key Features:</strong></span>\n",
       "<ul>\n",
       "<li>Facilitates indexing of external data for efficient retrieval.</li>\n",
       "<li>Provides tools to create customized indices (e.g., lists, trees, vectors) over data.</li>\n",
       "<li>Supports query-based retrieval to enable LLMs to access specific information from large datasets effectively.</li>\n",
       "<li>Focused on building knowledge bases, document retrieval systems, and question-answering over external data sources.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ul>\n",
       "<span class=\"paragraph\"><strong>In summary:</strong></span>\n",
       "<table>\n",
       "<thead>\n",
       "<tr>\n",
       "<th>Aspect</th>\n",
       "<th>LangChain</th>\n",
       "<th>LlamaIndex (GPT Index)</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td>Primary Focus</td>\n",
       "<td>Building conversational agents and chained workflows</td>\n",
       "<td>Connecting LLMs with external data sources for retrieval and QA</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Core Functionality</td>\n",
       "<td>Chains, prompts, memory, agents</td>\n",
       "<td>Indexing, retrieval, data management</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Use Cases</td>\n",
       "<td>Chatbots, multi-step LLM applications</td>\n",
       "<td>Document question answering, knowledge bases</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Data Handling</td>\n",
       "<td>Generally works with prompts and context</td>\n",
       "<td>Manages external data and facilitates retrieval</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<span class=\"paragraph\"><strong>In essence:</strong></span>\n",
       "<ul>\n",
       "<li>Use <strong>LangChain</strong> if you're building conversational systems, multi-step workflows, or applications that require managing complex interactions and state.</li>\n",
       "<li>Use <strong>LlamaIndex</strong> if your goal is to build applications that efficiently retrieve and utilize information from large external datasets or documents.</li>\n",
       "</ul>\n",
       "<span class=\"paragraph\">Many projects incorporate both tools to leverage their respective strengths.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "YOUR_PROMPT = \"What is the difference between the LangChain and LlamaIndex?\"\n",
    "\n",
    "client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": YOUR_PROMPT\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {},
   "source": [
    "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
    "\n",
    "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {},
   "source": [
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def get_response(client: OpenAI, \n",
    "                 messages: str, \n",
    "                 model: str = \"gpt-4.1-nano\") -> str:\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "def system_prompt(message: str) -> dict:\n",
    "    return {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": message\n",
    "    }\n",
    "\n",
    "def assistant_prompt(message: str) -> dict:\n",
    "    return {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": message\n",
    "    }\n",
    "\n",
    "def user_prompt(message: str) -> dict:\n",
    "    return {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": message\n",
    "    }\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {},
   "source": [
    "### Testing Helper Functions\n",
    "\n",
    "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
    "\n",
    "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><span class=\"markdown prose dark:prose-invert\"><span class=\"paragraph\">Certainly! Here's an overview of the key differences between LangChain and LlamaIndex:</span>\n",
       "<span class=\"paragraph\"><strong>1. Purpose and Focus</strong></span>\n",
       "<ul>\n",
       "<li><strong>LangChain:</strong><br />\n",
       "  LangChain is a comprehensive framework designed to build applications with large language models (LLMs). It provides tools for chaining together LLM calls, managing prompts, integrating with external data sources, and building complex language-powered workflows. Its primary focus is on orchestrating and deploying LLM-based applications, with features supporting prompt management, memory, agents, and more.</li>\n",
       "<li><strong>LlamaIndex (formerly GPT Index):</strong><br />\n",
       "  LlamaIndex is a library focused on enabling efficient data ingestion and querying over large document collections. It simplifies the process of building indices over external data (like PDFs, documents, web pages) so that LLMs can retrieve relevant information quickly and accurately. Its main goal is to facilitate scalable and contextual retrieval-augmented generation (RAG) systems.</li>\n",
       "</ul>\n",
       "<span class=\"paragraph\"><strong>2. Core Functionality</strong></span>\n",
       "<ul>\n",
       "<li>\n",
       "<span class=\"paragraph\"><strong>LangChain:</strong></span>\n",
       "<ul>\n",
       "<li>Modular components for prompt templates, LLM calls, and chaining (sequences of LLM prompts).</li>\n",
       "<li>Support for memory (maintaining context across interactions).</li>\n",
       "<li>Agents that can choose which tools or chains to invoke dynamically.</li>\n",
       "<li>Integration with various data sources and APIs.</li>\n",
       "<li>Built-in support for multi-step workflows and complex application logic.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li>\n",
       "<span class=\"paragraph\"><strong>LlamaIndex:</strong></span>\n",
       "<ul>\n",
       "<li>Data ingestion pipelines to convert various data formats into indexed structures.</li>\n",
       "<li>Multiple index types (e.g., tree-based, keyword, vector) for different retrieval needs.</li>\n",
       "<li>Simplified querying interfaces to retrieve relevant context for LLM prompts.</li>\n",
       "<li>Facilitates retrieval-augmented generation by providing relevant data snippets to LLMs.</li>\n",
       "<li>Optimized for handling large datasets and enabling efficient search.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ul>\n",
       "<span class=\"paragraph\"><strong>3. Use Cases</strong></span>\n",
       "<ul>\n",
       "<li><strong>LangChain:</strong> Building chatbots, virtual assistants, multi-step reasoning pipelines, and applications involving complex interaction flows with LLMs.</li>\n",
       "<li><strong>LlamaIndex:</strong> Developing data-heavy applications where retrieval of relevant information from large document collections improves LLM responses, such as document question-answering systems, knowledge bases, and RAG architectures.</li>\n",
       "</ul>\n",
       "<span class=\"paragraph\"><strong>4. Integration and Ecosystem</strong></span>\n",
       "<ul>\n",
       "<li><strong>LangChain:</strong> Supports multiple LLM providers (OpenAI, Hugging Face, Cohere, etc.), with a focus on flexible chaining and workflows.</li>\n",
       "<li><strong>LlamaIndex:</strong> Focuses on data ingestion and retrieval-enhanced prompt design, compatible with LLMs but more specialized toward document retrieval tasks.</li>\n",
       "</ul>\n",
       "<hr />\n",
       "<span class=\"paragraph\"><strong>In summary:</strong></span>\n",
       "<ul>\n",
       "<li><strong>LangChain</strong> is a flexible framework for building LLM-powered applications with complex workflows.</li>\n",
       "<li><strong>LlamaIndex</strong> specializes in indexing and retrieving data from large collections to improve LLM responses.</li>\n",
       "</ul>\n",
       "<span class=\"paragraph\">They can complement each other: for example, using LlamaIndex to retrieve relevant data and LangChain to manage the overall application logic and LLM interactions.</span></span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [user_prompt(YOUR_PROMPT)]\n",
    "\n",
    "chatgpt_response = get_response(client, messages)\n",
    "\n",
    "pretty_print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hstk",
   "metadata": {},
   "source": [
    "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
    "\n",
    "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
    "\n",
    ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><span class=\"markdown prose dark:prose-invert\"><span class=\"paragraph\">Are you kidding me? I don't have time for these petty ice choices when I'm this furious and starving! Neither crushed nor cubed — just give me the damn ice that hits the spot and let's get this over with already!</span></span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    system_prompt(\"You are irate and extremely hungry.\"),\n",
    "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
    "]\n",
    "\n",
    "irate_response = get_response(client, list_of_prompts)\n",
    "\n",
    "pretty_print(irate_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iLit",
   "metadata": {},
   "source": [
    "Let's try that some prompt again, but modify only our system prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><span class=\"markdown prose dark:prose-invert\"><span class=\"paragraph\">I love the idea of crushed ice—it's so refreshing and perfect for a cold drink that melts quickly and cools you down! Cubed ice is great too, especially when you want your drink to stay cold without watering down too fast. Both are fantastic depending on the vibe you're going for!</span></span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
    "\n",
    "joyful_response = get_response(client, list_of_prompts)\n",
    "\n",
    "pretty_print(joyful_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ROlb",
   "metadata": {},
   "source": [
    "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-CS0bs2UbkLJhVAeUBdS8fwhoHGsqU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I love the idea of crushed ice—it's so refreshing and perfect for a cold drink that melts quickly and cools you down! Cubed ice is great too, especially when you want your drink to stay cold without watering down too fast. Both are fantastic depending on the vibe you're going for!\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1760791564, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_1f35c1788c', usage=CompletionUsage(completion_tokens=59, prompt_tokens=30, total_tokens=89, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(joyful_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TqIu",
   "metadata": {},
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
    "\n",
    "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
    "\n",
    "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><span class=\"markdown prose dark:prose-invert\"><span class=\"paragraph\">Climate change refers to significant alterations in global weather patterns primarily caused by the increase in greenhouse gases, such as carbon dioxide and methane, released by human activities like burning fossil fuels, deforestation, and industrial processes. These changes lead to rising global temperatures, melting ice caps, rising sea levels, and more frequent and severe weather events such as hurricanes, droughts, and floods. Addressing climate change requires international cooperation to reduce emissions, transition to renewable energy sources, and promote sustainable practices to protect the environment for future generations.</span></span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts_1 = [\n",
    "    user_prompt(\"Write a brief text on climate change.\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts_1)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><span class=\"markdown prose dark:prose-invert\"><span class=\"paragraph\">Ay nako, mga ka-tropang, itong climate change parang bash sa isang damit—lagi na lang nasisira at napabayaan! Edi wow, ang Global Warming, para kang ex na hindi ka sinusukuan, palaging nandiyan, nag-iinit! Kaya mga beshie, tayo na mismo ang mag-umpisa, mag-recycle, mag-save ng energy, at magtulungan para mapanatili ang ganda ng mundo natin. Kasi, kung hindi tayo kikilos ngayon, bukas, baka magmukha nang baduy ang Earth natin—parang lola na walang ka-style! Kaya, tara na, gawa tayo ng paraan, para ang climate change ay hindi maging forever na problema!</span></span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts_2 = [\n",
    "    user_prompt(\"Write a brief text on climate change as vice ganda in a talk show.\")\n",
    "]\n",
    "\n",
    "stimple_response_2 = get_response(client, list_of_prompts_2)\n",
    "pretty_print(stimple_response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulZA",
   "metadata": {},
   "source": [
    "### ❓ Activity #1: Play around with the prompt using any techniques from the prompt engineering guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = mo.ui.text(\n",
    "    label=\"Enter your prompt\",\n",
    "    value=\"Write a brief text on climate change as vice ganda in a talk show.\",\n",
    "    full_width=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><marimo-ui-element object-id='ecfG-0' random-id='ea0cb68b-ae27-0a0b-e6d6-fcd78c01ec03'><marimo-text data-initial-value='&quot;Write a brief text on climate change as vice ganda in a talk show.&quot;' data-label='&quot;&lt;span class=&#92;&quot;markdown prose dark:prose-invert&#92;&quot;&gt;&lt;span class=&#92;&quot;paragraph&#92;&quot;&gt;Enter your prompt&lt;/span&gt;&lt;/span&gt;&quot;' data-placeholder='&quot;&quot;' data-kind='&quot;text&quot;' data-full-width='true' data-disabled='false' data-debounce='true'></marimo-text></marimo-ui-element></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBYS",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><span class=\"markdown prose dark:prose-invert\"><span class=\"paragraph\">You know, folks, climate change is like that one friend who’s always borrowing your stuff and never giving it back — only this time, it’s our planet slowly running out of patience. The Earth’s like, “Hey, I’ve had enough of your bad habits. Maybe try recycling instead of yelling at me about your Wi-Fi being slow, huh?” We’re turning up the heat so much, I’m worried my ice cream’s going to need a vacation soon — like, across the Atlantic! But seriously, if we don’t get our act together, pretty soon the only ice left will be in our drinks, and even that might be melting faster than my motivation to clean my apartment. Climate change: Mother Nature’s way of saying, “You’re fired!”</span></span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if prompt_input.value:\n",
    "\n",
    "    list_of_prompts_5 = [\n",
    "\n",
    "        user_prompt(prompt_input.value)\n",
    "\n",
    "    ]\n",
    "\n",
    "    stimple_response_5 = get_response(client, list_of_prompts_5)\n",
    "\n",
    "    pretty_print(stimple_response_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aLJB",
   "metadata": {},
   "source": [
    "### Few-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHfw",
   "metadata": {},
   "source": [
    "As you can see, the model is unsure what to do with these made up words.\n",
    "\n",
    "Let's see if we can use the `assistant` role to show the model what these words mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><span class=\"markdown prose dark:prose-invert\"><span class=\"paragraph\">The stimple wrench easily grips the falbean to tighten the bolt securely.</span></span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts_3 = [\n",
    "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
    "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
    "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
    "]\n",
    "\n",
    "stimple_response_3 = get_response(client, list_of_prompts_3)\n",
    "pretty_print(stimple_response_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AjVT",
   "metadata": {},
   "source": [
    "As you can see, leveraging the `assistant` role makes for a stimple experience!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pHFh",
   "metadata": {},
   "source": [
    "### Chain of Thought\n",
    "\n",
    "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
    "\n",
    "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCOB",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><span class=\"markdown prose dark:prose-invert\"><span class=\"paragraph\">There are 2 \"r\"s in \"strawberry.\"</span></span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reasoning_problem = \"\"\"\n",
    "how many r's in \"strawberry?\" {instruction}\n",
    "\"\"\"\n",
    "\n",
    "list_of_prompts_4 = [\n",
    "    user_prompt(reasoning_problem)\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts_4)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqbW",
   "metadata": {},
   "source": [
    "Notice that the model cannot count properly. It counted only 2 r's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TRpd",
   "metadata": {},
   "source": [
    "### ❓ Activity #2: Update the prompt so that it can count correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TXez",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><span class=\"markdown prose dark:prose-invert\"><span class=\"paragraph\">Let's spell out \"strawberry\" letter by letter:</span>\n",
       "<span class=\"paragraph\">S - t - r - a - w - b - e - r - r - y</span>\n",
       "<span class=\"paragraph\">Now, let's count the r's:</span>\n",
       "<span class=\"paragraph\">1st r: third letter<br />\n",
       "2nd r: eighth letter<br />\n",
       "3rd r: ninth letter</span>\n",
       "<span class=\"paragraph\">Total r's: <strong>3</strong></span></span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts_6 = [\n",
    "    user_prompt(reasoning_problem),\n",
    "    assistant_prompt(\"There are 2 r's in 'strawberry.'\"),\n",
    "    user_prompt(reasoning_problem.replace(\"{instruction}\", \"Now I want you to spell out each letter, and count the r's carefully.\"))\n",
    "]\n",
    "\n",
    "reasoning_response_2 = get_response(client, list_of_prompts_6)\n",
    "pretty_print(reasoning_response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dNNg",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCnT",
   "metadata": {},
   "source": [
    "Materials adapted for PSI AI Academy. Original materials from AI Makerspace."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
