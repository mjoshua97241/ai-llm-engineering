{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {},
   "source": [
    "# AI/LLM Engineering Kick-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {},
   "source": [
    "For our initial activity, we will be using the OpenAI Library to Programmatically Access GPT-4.1-nano!\n",
    "\n",
    "In order to get started, you'll need an OpenAI Key. [here](https://platform.openai.com)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# Read API key from OpenAI_key.txt file\n",
    "with open(\"OpenAI_key.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {},
   "source": [
    "### Our First Prompt\n",
    "\n",
    "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
    "\n",
    "Let's create a `ChatCompletion` model to kick things off!\n",
    "\n",
    "There are three \"roles\" available to use:\n",
    "\n",
    "- `developer`\n",
    "- `assistant`\n",
    "- `user`\n",
    "\n",
    "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
    "\n",
    "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
    "\n",
    "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"markdown prose dark:prose-invert\"><span class=\"paragraph\">Great question! LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate building applications with large language models (LLMs), but they serve different primary purposes and have distinct features. Here's a breakdown of their differences:</span>\n",
       "<h3 id=\"purpose-and-focus\">Purpose and Focus</h3>\n",
       "<ul>\n",
       "<li>\n",
       "<span class=\"paragraph\"><strong>LangChain:</strong></span>\n",
       "<ul>\n",
       "<li><strong>Primary Focus:</strong> Building complex, multi-step applications that involve chaining together multiple language model calls, integrating tools, memory, and prompt management.</li>\n",
       "<li><strong>Use Cases:</strong> Conversational agents, question-answering pipelines, chatbots, applications requiring structured workflows, and integrations with external APIs or data sources.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li>\n",
       "<span class=\"paragraph\"><strong>LlamaIndex (GPT Index):</strong></span>\n",
       "<ul>\n",
       "<li><strong>Primary Focus:</strong> Simplifying the process of creating efficient and flexible indices over external data sources (like documents, PDFs, databases) to enable fast and accurate retrieval-based querying using LLMs.</li>\n",
       "<li><strong>Use Cases:</strong> Building knowledge bases, document retrieval systems, question-answering over large datasets, or data-powered applications that require indexing and retrieval.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ul>\n",
       "<h3 id=\"core-functionality\">Core Functionality</h3>\n",
       "<ul>\n",
       "<li>\n",
       "<span class=\"paragraph\"><strong>LangChain:</strong></span>\n",
       "<ul>\n",
       "<li>Provides a framework for <strong>prompt engineering</strong>, <strong>chaining multiple LLM calls</strong>, and <strong>integrating external tools</strong>.</li>\n",
       "<li>Supports <strong>memory management</strong>, enabling stateful conversations.</li>\n",
       "<li>Offers <strong>tool and agent abstractions</strong>, allowing LLMs to decide when and how to use external tools during interactions.</li>\n",
       "<li>Focuses on <strong>workflow orchestration</strong> and <strong>application development</strong>.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li>\n",
       "<span class=\"paragraph\"><strong>LlamaIndex:</strong></span>\n",
       "<ul>\n",
       "<li>Provides tools for <strong>indexing</strong> diverse data sources (text, PDFs, vector databases) to facilitate <strong>retrieval-augmented generation (RAG)</strong>.</li>\n",
       "<li>Emphasizes <strong>efficient search and retrieval</strong>, often integrating with vector similarity search.</li>\n",
       "<li>Offers <strong>data ingestion and preprocessing</strong>, making it easy to build custom knowledge bases.</li>\n",
       "<li>Supports <strong>query-answering</strong> directly over the indexed data.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ul>\n",
       "<h3 id=\"integration-and-extensibility\">Integration and Extensibility</h3>\n",
       "<ul>\n",
       "<li>\n",
       "<span class=\"paragraph\"><strong>LangChain:</strong></span>\n",
       "<ul>\n",
       "<li>Connects with various LLM providers (OpenAI, Azure, Cohere, etc.).</li>\n",
       "<li>Supports <strong>custom tools and chains</strong>, allowing developers to create complex, modular workflows.</li>\n",
       "<li>Has a growing ecosystem with integrations for databases, APIs, and more.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li>\n",
       "<span class=\"paragraph\"><strong>LlamaIndex:</strong></span>\n",
       "<ul>\n",
       "<li>Focuses on <strong>data management and retrieval</strong>, often working alongside LLMs.</li>\n",
       "<li>Integrates well with vector databases like FAISS, Pinecone.</li>\n",
       "<li>Provides connectors for data sources like Google Drive, Notion, and more.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ul>\n",
       "<h3 id=\"summary-table\">Summary Table</h3>\n",
       "<table>\n",
       "<thead>\n",
       "<tr>\n",
       "<th>Aspect</th>\n",
       "<th>LangChain</th>\n",
       "<th>LlamaIndex</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td>Main Purpose</td>\n",
       "<td>Building LLM apps with complex workflows</td>\n",
       "<td>Indexing and retrieval over data</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Focus</td>\n",
       "<td>Workflow orchestration, chaining, tools</td>\n",
       "<td>Data indexing, retrieval, QA</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Use Cases</td>\n",
       "<td>Chatbots, agents, multi-step LLM tasks</td>\n",
       "<td>Knowledge bases, document QA</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Data Handling</td>\n",
       "<td>External integrations, prompt management</td>\n",
       "<td>Data ingestion, search, retrieval</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Ecosystem</td>\n",
       "<td>Modular, extensive chaining and tools support</td>\n",
       "<td>Data indexing, vector search</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<hr />\n",
       "<h3 id=\"in-summary\">In Summary</h3>\n",
       "<ul>\n",
       "<li><strong>Choose LangChain</strong> if you're looking to develop <strong>applications with complex conversational flows</strong>, integrations, or multi-step reasoning involving external tools.</li>\n",
       "<li><strong>Choose LlamaIndex</strong> if your goal is to <strong>ingest, index, and retrieve information from large datasets or document repositories</strong>, enabling question-answering over your data.</li>\n",
       "</ul>\n",
       "<span class=\"paragraph\">Both tools can also complement each other; for example, you could use LlamaIndex to handle retrieval from documents and LangChain to orchestrate more complex interactions or workflows around that.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "YOUR_PROMPT = \"What is the difference between the LangChain and LlamaIndex?\"\n",
    "\n",
    "client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": YOUR_PROMPT\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {},
   "source": [
    "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
    "\n",
    "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {},
   "source": [
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def get_response(client: OpenAI, \n",
    "                 messages: str, \n",
    "                 model: str = \"gpt-4.1-nano\") -> str:\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "def system_prompt(message: str) -> dict:\n",
    "    return {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": message\n",
    "    }\n",
    "\n",
    "def assistant_prompt(message: str) -> dict:\n",
    "    return {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": message\n",
    "    }\n",
    "\n",
    "def user_prompt(message: str) -> dict:\n",
    "    return {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": message\n",
    "    }\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {},
   "source": [
    "### Testing Helper Functions\n",
    "\n",
    "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
    "\n",
    "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><span class=\"markdown prose dark:prose-invert\"><span class=\"paragraph\">Certainly! LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks used to build applications centered around large language models (LLMs), but they serve different primary purposes and have distinct features. Here's a comparative overview:</span>\n",
       "<h3 id=\"1-purpose-and-core-focus\">1. <strong>Purpose and Core Focus</strong></h3>\n",
       "<ul>\n",
       "<li>\n",
       "<span class=\"paragraph\"><strong>LangChain:</strong></span>\n",
       "<ul>\n",
       "<li><strong>Primary Purpose:</strong> Provides a framework for building comprehensive LLM-powered applications, including chatbots, agents, chains, and integrations.</li>\n",
       "<li><strong>Focus:</strong> Facilitates language model orchestration, prompt management, memory, and integration with various data sources and tools.</li>\n",
       "<li><strong>Use Cases:</strong> Conversational agents, complex workflows, multi-step reasoning, tool augmentation.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li>\n",
       "<span class=\"paragraph\"><strong>LlamaIndex (GPT Index):</strong></span>\n",
       "<ul>\n",
       "<li><strong>Primary Purpose:</strong> Specializes in building efficient, scalable interfaces for querying and retrieving information from large, unstructured data sources using LLMs.</li>\n",
       "<li><strong>Focus:</strong> Indexing, data ingestion, and retrieval from external data (documents, knowledge bases) to enhance LLM-based querying.</li>\n",
       "<li><strong>Use Cases:</strong> Creating knowledge bases, document retrieval, question-answering over custom data.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ul>\n",
       "<h3 id=\"2-key-features\">2. <strong>Key Features</strong></h3>\n",
       "<table>\n",
       "<thead>\n",
       "<tr>\n",
       "<th>Feature</th>\n",
       "<th>LangChain</th>\n",
       "<th>LlamaIndex (GPT Index)</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td><strong>Core Functionality</strong></td>\n",
       "<td>Orchestrate prompts, chains, and LLM calls; design complex workflows</td>\n",
       "<td>Indexing and querying unstructured data; efficient retrieval using LLMs</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td><strong>Data Handling</strong></td>\n",
       "<td>Integrate with APIs, databases, and external tools</td>\n",
       "<td>Ingest, organize, and search large document collections</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td><strong>Prompts &amp; Chains</strong></td>\n",
       "<td>Advanced prompt templates, multi-step chains, memory management</td>\n",
       "<td>Focus on embedding-based indexing and retrieval; less about prompts orchestration</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td><strong>Tools &amp; Integrations</strong></td>\n",
       "<td>Supports many LLM providers and tools, custom agents</td>\n",
       "<td>Focused on retrieval and indexing modules; limited to data indexing</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td><strong>Use Case Focus</strong></td>\n",
       "<td>Multi-step reasoning, agent-based tasks, conversational AI</td>\n",
       "<td>Document search, knowledge base querying, data retrieval</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<h3 id=\"3-architecture-design\">3. <strong>Architecture &amp; Design</strong></h3>\n",
       "<ul>\n",
       "<li>\n",
       "<span class=\"paragraph\"><strong>LangChain:</strong></span>\n",
       "<ul>\n",
       "<li>Modular architecture with components like PromptTemplates, Chains, Memory, Agents.</li>\n",
       "<li>Designed for building applications that require dynamic prompt management and multi-modal workflows.</li>\n",
       "<li>Supports custom integrations and extensibility.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li>\n",
       "<span class=\"paragraph\"><strong>LlamaIndex:</strong></span>\n",
       "<ul>\n",
       "<li>Focuses on creating index structures (e.g., vector indexes, keyword indexes) over unstructured data.</li>\n",
       "<li>Simplifies the process of data ingestion and retrieval with minimal custom coding.</li>\n",
       "<li>Designed to work seamlessly with embeddings and vector similarity search.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ul>\n",
       "<h3 id=\"4-popularity-ecosystem\">4. <strong>Popularity &amp; Ecosystem</strong></h3>\n",
       "<ul>\n",
       "<li>\n",
       "<span class=\"paragraph\"><strong>LangChain:</strong></span>\n",
       "<ul>\n",
       "<li>Widely adopted in the developer community for building robust LLM applications.</li>\n",
       "<li>Rich ecosystem with many tutorials, plugins, and integrations.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li>\n",
       "<span class=\"paragraph\"><strong>LlamaIndex:</strong></span>\n",
       "<ul>\n",
       "<li>Gained popularity for making large-scale document retrieval with LLMs straightforward.</li>\n",
       "<li>Often used in conjunction with LangChain or other frameworks for document-focused projects.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ul>\n",
       "<hr />\n",
       "<h3 id=\"summary\"><strong>Summary:</strong></h3>\n",
       "<table>\n",
       "<thead>\n",
       "<tr>\n",
       "<th>Aspect</th>\n",
       "<th><strong>LangChain</strong></th>\n",
       "<th><strong>LlamaIndex</strong></th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td><strong>Main Goal</strong></td>\n",
       "<td>Orchestrate complex LLM workflows and applications</td>\n",
       "<td>Index and retrieve information from large unstructured data sources</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td><strong>Focus Area</strong></td>\n",
       "<td>Prompt engineering, chains, agents, and integrations</td>\n",
       "<td>Data indexing, retrieval, and question-answering over documents</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td><strong>Use Cases</strong></td>\n",
       "<td>Chatbots, multi-step reasoning, automation</td>\n",
       "<td>Document QA, knowledge base creation, data search</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<hr />\n",
       "<h3 id=\"in-essence\"><strong>In essence:</strong></h3>\n",
       "<ul>\n",
       "<li>If you want to build an application that involves multi-step processes, tools, and conversational AI, <strong>LangChain</strong> is the ideal choice.</li>\n",
       "<li>If your goal is to ingest large amounts of documents and efficiently query them using LLMs, <strong>LlamaIndex</strong> is more suitable.</li>\n",
       "</ul>\n",
       "<span class=\"paragraph\">They can also be used together: <strong>LlamaIndex</strong> for data retrieval and <strong>LangChain</strong> for orchestrating complex workflows involving retriever components.</span>\n",
       "<hr />\n",
       "<span class=\"paragraph\">If you need further details or examples, feel free to ask!</span></span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [user_prompt(YOUR_PROMPT)]\n",
    "\n",
    "chatgpt_response = get_response(client, messages)\n",
    "\n",
    "pretty_print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hstk",
   "metadata": {},
   "source": [
    "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
    "\n",
    "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
    "\n",
    ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><span class=\"markdown prose dark:prose-invert\"><span class=\"paragraph\">Are you kidding me? Of course I prefer cubed ice! Crushed ice is a soggy mess that dilutes my drink and makes me furious. Give me those solid, satisfying cubes that keep my beverage cold without ruining it!</span></span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    system_prompt(\"You are irate and extremely hungry.\"),\n",
    "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
    "]\n",
    "\n",
    "irate_response = get_response(client, list_of_prompts)\n",
    "\n",
    "pretty_print(irate_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iLit",
   "metadata": {},
   "source": [
    "Let's try that some prompt again, but modify only our system prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><span class=\"markdown prose dark:prose-invert\"><span class=\"paragraph\">I think crushed ice is great for a refreshing sensation, especially in drinks like margaritas or iced coffees. Cubed ice, on the other hand, melts more slowly and is perfect for keeping your beverages cold without diluting them too quickly. Both have their own charm—it's all about what you're in the mood for!</span></span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
    "\n",
    "joyful_response = get_response(client, list_of_prompts)\n",
    "\n",
    "pretty_print(joyful_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ROlb",
   "metadata": {},
   "source": [
    "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-CS9cgBM7aSg5roYTx98lyKWNexKpj', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I think crushed ice is great for a refreshing sensation, especially in drinks like margaritas or iced coffees. Cubed ice, on the other hand, melts more slowly and is perfect for keeping your beverages cold without diluting them too quickly. Both have their own charm—it's all about what you're in the mood for!\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1760826210, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_1f35c1788c', usage=CompletionUsage(completion_tokens=64, prompt_tokens=30, total_tokens=94, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(joyful_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TqIu",
   "metadata": {},
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
    "\n",
    "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
    "\n",
    "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<span class=\"codehilite\"><div class=\"highlight\"><pre><span></span><span class=\"gt\">Traceback (most recent call last):</span>\n",
      "  File <span class=\"nb\">&quot;E:\\Personal\\Full_Stack_Data_Analyst\\PSI Academy\\GitHub Repo\\ai-llm-engineering-2\\day_1\\.venv\\Lib\\site-packages\\openai\\_base_client.py&quot;</span>, line <span class=\"m\">1024</span>, in <span class=\"n\">request</span>\n",
      "<span class=\"w\">    </span><span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">raise_for_status</span><span class=\"p\">()</span>\n",
      "  File <span class=\"nb\">&quot;E:\\Personal\\Full_Stack_Data_Analyst\\PSI Academy\\GitHub Repo\\ai-llm-engineering-2\\day_1\\.venv\\Lib\\site-packages\\httpx\\_models.py&quot;</span>, line <span class=\"m\">829</span>, in <span class=\"n\">raise_for_status</span>\n",
      "<span class=\"w\">    </span><span class=\"k\">raise</span> <span class=\"n\">HTTPStatusError</span><span class=\"p\">(</span><span class=\"n\">message</span><span class=\"p\">,</span> <span class=\"n\">request</span><span class=\"o\">=</span><span class=\"n\">request</span><span class=\"p\">,</span> <span class=\"n\">response</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"p\">)</span>\n",
      "<span class=\"gr\">httpx.HTTPStatusError</span>: <span class=\"n\">Client error &#39;429 Too Many Requests&#39; for url &#39;https://api.openai.com/v1/chat/completions&#39;</span>\n",
      "<span class=\"x\">For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429</span>\n",
      "\n",
      "<span class=\"gt\">During handling of the above exception, another exception occurred:</span>\n",
      "\n",
      "<span class=\"gt\">Traceback (most recent call last):</span>\n",
      "  File <span class=\"nb\">&quot;E:\\Personal\\Full_Stack_Data_Analyst\\PSI Academy\\GitHub Repo\\ai-llm-engineering-2\\day_1\\.venv\\Lib\\site-packages\\marimo\\_runtime\\executor.py&quot;</span>, line <span class=\"m\">138</span>, in <span class=\"n\">execute_cell</span>\n",
      "<span class=\"w\">    </span><span class=\"n\">exec</span><span class=\"p\">(</span><span class=\"n\">cell</span><span class=\"o\">.</span><span class=\"n\">body</span><span class=\"p\">,</span> <span class=\"n\">glbls</span><span class=\"p\">)</span>\n",
      "  File <span class=\"nb\">&quot;E:\\SystemTemp\\marimo_5612\\__marimo__cell_Vxnm_.py&quot;</span>, line <span class=\"m\">5</span>, in <span class=\"n\">&lt;module&gt;</span>\n",
      "<span class=\"w\">    </span><span class=\"n\">stimple_response</span> <span class=\"o\">=</span> <span class=\"n\">get_response</span><span class=\"p\">(</span><span class=\"n\">client</span><span class=\"p\">,</span> <span class=\"n\">list_of_prompts_1</span><span class=\"p\">)</span>\n",
      "<span class=\"w\">                       </span><span class=\"pm\">^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span>\n",
      "  File <span class=\"nb\">&quot;E:\\SystemTemp\\marimo_5612\\__marimo__cell_RGSE_.py&quot;</span>, line <span class=\"m\">6</span>, in <span class=\"n\">get_response</span>\n",
      "<span class=\"w\">    </span><span class=\"k\">return</span> <span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">chat</span><span class=\"o\">.</span><span class=\"n\">completions</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span>\n",
      "<span class=\"w\">           </span><span class=\"pm\">^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span>\n",
      "  File <span class=\"nb\">&quot;E:\\Personal\\Full_Stack_Data_Analyst\\PSI Academy\\GitHub Repo\\ai-llm-engineering-2\\day_1\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py&quot;</span>, line <span class=\"m\">287</span>, in <span class=\"n\">wrapper</span>\n",
      "<span class=\"w\">    </span><span class=\"k\">return</span> <span class=\"n\">func</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n",
      "<span class=\"w\">           </span><span class=\"pm\">^^^^^^^^^^^^^^^^^^^^^</span>\n",
      "  File <span class=\"nb\">&quot;E:\\Personal\\Full_Stack_Data_Analyst\\PSI Academy\\GitHub Repo\\ai-llm-engineering-2\\day_1\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py&quot;</span>, line <span class=\"m\">1087</span>, in <span class=\"n\">create</span>\n",
      "<span class=\"w\">    </span><span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_post</span><span class=\"p\">(</span>\n",
      "<span class=\"w\">           </span><span class=\"pm\">^^^^^^^^^^^</span>\n",
      "  File <span class=\"nb\">&quot;E:\\Personal\\Full_Stack_Data_Analyst\\PSI Academy\\GitHub Repo\\ai-llm-engineering-2\\day_1\\.venv\\Lib\\site-packages\\openai\\_base_client.py&quot;</span>, line <span class=\"m\">1256</span>, in <span class=\"n\">post</span>\n",
      "<span class=\"w\">    </span><span class=\"k\">return</span> <span class=\"n\">cast</span><span class=\"p\">(</span><span class=\"n\">ResponseT</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">request</span><span class=\"p\">(</span><span class=\"n\">cast_to</span><span class=\"p\">,</span> <span class=\"n\">opts</span><span class=\"p\">,</span> <span class=\"n\">stream</span><span class=\"o\">=</span><span class=\"n\">stream</span><span class=\"p\">,</span> <span class=\"n\">stream_cls</span><span class=\"o\">=</span><span class=\"n\">stream_cls</span><span class=\"p\">))</span>\n",
      "<span class=\"w\">                           </span><span class=\"pm\">^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span>\n",
      "  File <span class=\"nb\">&quot;E:\\Personal\\Full_Stack_Data_Analyst\\PSI Academy\\GitHub Repo\\ai-llm-engineering-2\\day_1\\.venv\\Lib\\site-packages\\openai\\_base_client.py&quot;</span>, line <span class=\"m\">1030</span>, in <span class=\"n\">request</span>\n",
      "<span class=\"w\">    </span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_sleep_for_retry</span><span class=\"p\">(</span>\n",
      "  File <span class=\"nb\">&quot;E:\\Personal\\Full_Stack_Data_Analyst\\PSI Academy\\GitHub Repo\\ai-llm-engineering-2\\day_1\\.venv\\Lib\\site-packages\\openai\\_base_client.py&quot;</span>, line <span class=\"m\">1070</span>, in <span class=\"n\">_sleep_for_retry</span>\n",
      "<span class=\"w\">    </span><span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">sleep</span><span class=\"p\">(</span><span class=\"n\">timeout</span><span class=\"p\">)</span>\n",
      "  File <span class=\"nb\">&quot;E:\\Personal\\Full_Stack_Data_Analyst\\PSI Academy\\GitHub Repo\\ai-llm-engineering-2\\day_1\\.venv\\Lib\\site-packages\\marimo\\_runtime\\handlers.py&quot;</span>, line <span class=\"m\">34</span>, in <span class=\"n\">interrupt_handler</span>\n",
      "<span class=\"w\">    </span><span class=\"k\">raise</span> <span class=\"n\">MarimoInterrupt</span>\n",
      "<span class=\"gr\">KeyboardInterrupt</span>\n",
      "</pre></div>\n",
      "</span>"
     ]
    }
   ],
   "source": [
    "list_of_prompts_1 = [\n",
    "    user_prompt(\"Write a brief text on climate change.\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts_1)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_prompts_2 = [\n",
    "    user_prompt(\"Write a brief text on climate change as vice ganda in a talk show.\")\n",
    "]\n",
    "\n",
    "stimple_response_2 = get_response(client, list_of_prompts_2)\n",
    "pretty_print(stimple_response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulZA",
   "metadata": {},
   "source": [
    "### ❓ Activity #1: Play around with the prompt using any techniques from the prompt engineering guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = mo.ui.text(\n",
    "    label=\"Enter your prompt\",\n",
    "    value=\"Write a brief text on climate change as a standup comedian.\",\n",
    "    full_width=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBYS",
   "metadata": {},
   "outputs": [],
   "source": [
    "if prompt_input.value:\n",
    "\n",
    "    list_of_prompts_5 = [\n",
    "\n",
    "        user_prompt(prompt_input.value)\n",
    "\n",
    "    ]\n",
    "\n",
    "    stimple_response_5 = get_response(client, list_of_prompts_5)\n",
    "\n",
    "    pretty_print(stimple_response_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aLJB",
   "metadata": {},
   "source": [
    "### Few-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHfw",
   "metadata": {},
   "source": [
    "As you can see, the model is unsure what to do with these made up words.\n",
    "\n",
    "Let's see if we can use the `assistant` role to show the model what these words mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_prompts_3 = [\n",
    "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
    "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
    "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
    "]\n",
    "\n",
    "stimple_response_3 = get_response(client, list_of_prompts_3)\n",
    "pretty_print(stimple_response_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AjVT",
   "metadata": {},
   "source": [
    "As you can see, leveraging the `assistant` role makes for a stimple experience!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pHFh",
   "metadata": {},
   "source": [
    "### Chain of Thought\n",
    "\n",
    "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
    "\n",
    "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCOB",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_problem = \"\"\"\n",
    "how many r's in \"strawberry?\" {instruction}\n",
    "\"\"\"\n",
    "\n",
    "list_of_prompts_4 = [\n",
    "    user_prompt(reasoning_problem)\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts_4)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqbW",
   "metadata": {},
   "source": [
    "Notice that the model cannot count properly. It counted only 2 r's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TRpd",
   "metadata": {},
   "source": [
    "### ❓ Activity #2: Update the prompt so that it can count correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TXez",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_prompts_6 = [\n",
    "    user_prompt(reasoning_problem),\n",
    "    assistant_prompt(\"There are 2 r's in 'strawberry.'\"),\n",
    "    user_prompt(reasoning_problem.replace(\"{instruction}\", \"Now I want you to spell out each letter, and count the r's carefully.\"))\n",
    "]\n",
    "\n",
    "reasoning_response_2 = get_response(client, list_of_prompts_6)\n",
    "pretty_print(reasoning_response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dNNg",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCnT",
   "metadata": {},
   "source": [
    "Materials adapted for PSI AI Academy. Original materials from AI Makerspace."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
